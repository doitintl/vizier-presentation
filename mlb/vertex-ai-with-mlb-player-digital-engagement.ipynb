{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-14T18:57:35.372476Z",
     "iopub.status.busy": "2021-06-14T18:57:35.371945Z",
     "iopub.status.idle": "2021-06-14T18:57:35.386745Z",
     "shell.execute_reply": "2021-06-14T18:57:35.384957Z",
     "shell.execute_reply.started": "2021-06-14T18:57:35.372429Z"
    }
   },
   "source": [
    "# Getting Started on Vertex AI Notebooks #\n",
    "\n",
    "This notebook demonstrates how to do the following on Vertex AI, Google's powerful new machine learning platform:\n",
    "\n",
    "- run the getting started notebook on Vertex AI Notebooks, to load the data, create a model & generate predictions\n",
    "- explore explainable AI on Vertex AI to refine your features\n",
    "- tune hyperparameters with Vizier\n",
    "\n",
    "It is a complement to the [Getting Started with MLB Digital Engagement](https://www.kaggle.com/ryanholbrook/getting-started-with-mlb-player-digital-engagement) tutorial which was designed to be run on Kaggle Notebooks.\n",
    "\n",
    "This tutorial uses Cloud Notebooks, a billable component of Google Cloud. Learn more about [Notebooks pricing](https://cloud.google.com/notebooks/pricing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Setup #\n",
    "\n",
    "\n",
    "### 1. Download this Notebook ###\n",
    "\n",
    "Start by creating your own copy of this notebook. Click the *Copy and Edit* button to the upper right. Now, in the menubar above, click *File -> Download Notebook* and save a copy of the notebook to your computer. We will reupload this in an AI Notebooks instance to take advantage of the Explainable AI service.\n",
    "\n",
    "### 2. Download Kaggle API Key ###\n",
    "We'll use the Kaggle API to download the competition data to the notebook instance. You'll need a copy of your Kaggle credentials to authenticate your account.\n",
    "\n",
    "From the site header, click on your user profile picture, then on “My Account” from the dropdown menu. This will take you to your account settings at https://www.kaggle.com/account. Scroll down to the section of the page labelled API.\n",
    "\n",
    "To create a new token, click on the “Create New API Token” button. This will download a fresh authentication token onto your machine.\n",
    "\n",
    "### 3. Sign up for Google Cloud Platform ###\n",
    "If you don't have a GCP account already, go to https://cloud.google.com/ and click on “Get Started For Free\". This is a two step sign up process where you will need to provide your name, address and a credit card. The starter account is free and it comes with $300 credit that you can use. For this step you will need to provide a Google Account (i.e. your Gmail account) to sign in.\n",
    "\n",
    "### 4. Create a Project and Enable the Notebook API ###\n",
    "Follow the directions at https://cloud.google.com/notebooks/docs/before-you-begin to setup a notebook project.\n",
    "\n",
    "### 5. Create a Notebook Instance ###\n",
    "Next, go to https://notebook.new. Enter an `Instance name` of your choice and then click the blue **CREATE** button at the end of the page. Be sure to keep the default `TensorFlow Enterprise` environment. You'll be redirected to a page with a list of your notebook instances. It may take a few minutes for the instance you just created to start up.\n",
    "\n",
    "Once the notebook instance is running, click `OPEN JUPYTERLAB` just to the right of the instance name. You should be redirected to a JupyterLab environment.\n",
    "\n",
    "### 6. Upload MLB Notebook and API Key ###\n",
    "From inside JupyterLab, click the \"Upload Files\" (up arrow) button in the file browser on the left and upload the files `kaggle.json` and `vertex-ai-with-mlb-player-digital-engagement.ipynb`.\n",
    "\n",
    "### 7. Authenticate Kaggle API and Download MLB Date ###\n",
    "\n",
    "Run the next cell to download the competition data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "pip install -q kaggle\n",
    "mkdir -p /home/jupyter/.kaggle\n",
    "mv kaggle.json /home/jupyter/.kaggle/kaggle.json 2>/dev/null\n",
    "chmod 600 /home/jupyter/.kaggle/kaggle.json\n",
    "\n",
    "kaggle competitions download mlb-player-digital-engagement-forecasting\n",
    "mkdir -p /home/jupyter/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  mlb-player-digital-engagement-forecasting.zip\n",
      "  inflating: ./input/mlb-player-digital-engagement-forecasting/awards.csv  \n",
      "  inflating: ./input/mlb-player-digital-engagement-forecasting/example_sample_submission.csv  \n",
      "  inflating: ./input/mlb-player-digital-engagement-forecasting/example_test.csv  \n",
      "  inflating: ./input/mlb-player-digital-engagement-forecasting/mlb/__init__.py  \n",
      "  inflating: ./input/mlb-player-digital-engagement-forecasting/mlb/competition.cpython-37m-x86_64-linux-gnu.so  \n",
      "  inflating: ./input/mlb-player-digital-engagement-forecasting/players.csv  \n",
      "  inflating: ./input/mlb-player-digital-engagement-forecasting/seasons.csv  \n",
      "  inflating: ./input/mlb-player-digital-engagement-forecasting/teams.csv  \n",
      "  inflating: ./input/mlb-player-digital-engagement-forecasting/train.csv  \n",
      "  inflating: ./input/mlb-player-digital-engagement-forecasting/train_updated.csv  \n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "#JF Fixed\n",
    "#unzip -u mlb-player-digital-engagement-forecasting.zip -d /home/jupyter/input/mlb-player-digital-engagement-forecasting\n",
    "mkdir input ||true\n",
    "unzip -u mlb-player-digital-engagement-forecasting.zip -d ./input/mlb-player-digital-engagement-forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Run this Tutorial ###\n",
    "\n",
    "After you've completed the setup steps above in Vertex AI Notebooks, select \"Run\" from the menubar and \"Run Selected Cell and All Below\" to run through the rest of this notebook automatically. Or step through cell-by-cell, if you'd prefer. Note that the Explainable AI (XAI) walkthrough is at the bottom of this notebook.\n",
    "\n",
    "### 8. Cleaning Up ###\n",
    "\n",
    "You'll be billed for any time you keep the notebook instance running. So after you're done, be sure either to stop the notebook instance from the *Notebooks* page, or else to delete the Cloud project you created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLB Getting Started #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part of this notebook reproduces the data and model setup of the [Getting Started](https://www.kaggle.com/ryanholbrook/getting-started-with-mlb-player-digital-engagement) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 6.311292,
     "end_time": "2021-06-07T23:22:37.716875",
     "exception": false,
     "start_time": "2021-06-07T23:22:31.405583",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import sys\n",
    "import warnings\n",
    "from joblib import Parallel, delayed\n",
    "from pathlib import Path\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.tsa.deterministic import (CalendarFourier,\n",
    "                                           CalendarSeasonality,\n",
    "                                           CalendarTimeTrend,\n",
    "                                           DeterministicProcess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 6.311292,
     "end_time": "2021-06-07T23:22:37.716875",
     "exception": false,
     "start_time": "2021-06-07T23:22:31.405583",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.layers.experimental.preprocessing import StringLookup\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# Set Matplotlib defaults\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "plt.rc(\"figure\", autolayout=True, figsize=(11, 5))\n",
    "plt.rc(\n",
    "    \"axes\",\n",
    "    labelweight=\"bold\",\n",
    "    labelsize=\"large\",\n",
    "    titleweight=\"bold\",\n",
    "    titlesize=14,\n",
    "    titlepad=10,\n",
    ")\n",
    "plot_params = dict(\n",
    "    color=\"0.75\",\n",
    "    style=\".-\",\n",
    "    markeredgecolor=\"0.25\",\n",
    "    markerfacecolor=\"0.25\",\n",
    "    legend=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_kg_hide-input": true,
    "papermill": {
     "duration": 0.266957,
     "end_time": "2021-06-07T23:22:38.003781",
     "exception": false,
     "start_time": "2021-06-07T23:22:37.736824",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper function to unpack json found in daily data\n",
    "def unpack_json(json_str):\n",
    "    return pd.DataFrame() if pd.isna(json_str) else pd.read_json(json_str)\n",
    "\n",
    "\n",
    "def unpack_data(data, dfs=None, n_jobs=-1):\n",
    "    if dfs is not None:\n",
    "        data = data.loc[:, dfs]\n",
    "    unnested_dfs = {}\n",
    "    for name, column in data.iteritems():\n",
    "        daily_dfs = Parallel(n_jobs=n_jobs)(\n",
    "            delayed(unpack_json)(item) for date, item in column.iteritems())\n",
    "        df = pd.concat(daily_dfs)\n",
    "        unnested_dfs[name] = df\n",
    "    return unnested_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba4d6d5ecee841448bd5e955a7cb63d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(Output(), Output(), Output(), Output()), _titles={'0': 'seasons', '1': 'teams', '2': 'players', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#JF\n",
    "data_dir = Path('./input/mlb-player-digital-engagement-forecasting/')\n",
    "\n",
    "df_names = ['seasons', 'teams', 'players', 'awards']\n",
    "\n",
    "for name in df_names:\n",
    "    globals()[name] = pd.read_csv(data_dir / f\"{name}.csv\")\n",
    "\n",
    "kaggle_data_tabs = widgets.Tab()\n",
    "# Add Output widgets for each pandas DF as tabs' children\n",
    "kaggle_data_tabs.children = list([widgets.Output() for df_name in df_names])\n",
    "\n",
    "for index in range(0, len(df_names)):\n",
    "    # Rename tab bar titles to df names\n",
    "    kaggle_data_tabs.set_title(index, df_names[index])\n",
    "    \n",
    "    # Display corresponding table output for this tab name\n",
    "    with kaggle_data_tabs.children[index]:\n",
    "        display(eval(df_names[index]))\n",
    "\n",
    "display(kaggle_data_tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_kg_hide-input": true,
    "papermill": {
     "duration": 106.821184,
     "end_time": "2021-06-07T23:24:24.850746",
     "exception": false,
     "start_time": "2021-06-07T23:22:38.029562",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "PeriodIndex: 1216 entries, 2018-01-01 to 2021-04-30\n",
      "Freq: D\n",
      "Data columns (total 2 columns):\n",
      " #   Column                   Non-Null Count  Dtype \n",
      "---  ------                   --------------  ----- \n",
      " 0   nextDayPlayerEngagement  1216 non-null   object\n",
      " 1   playerBoxScores          538 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 28.5+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define dataframes to load from training set\n",
    "dfs = [\n",
    "    'nextDayPlayerEngagement',  # targets\n",
    "    'playerBoxScores',  # features\n",
    "    # Other dataframes available for features:\n",
    "    # 'games',\n",
    "    # 'rosters',\n",
    "    # 'teamBoxScores',\n",
    "    # 'transactions',\n",
    "    # 'standings',\n",
    "    # 'awards',\n",
    "    # 'events',\n",
    "    # 'playerTwitterFollowers',\n",
    "    # 'teamTwitterFollowers',\n",
    "]\n",
    "\n",
    "# Read training data\n",
    "training = pd.read_csv(\n",
    "    data_dir / 'train.csv',\n",
    "    usecols=['date'] + dfs,\n",
    ")\n",
    "\n",
    "# Convert training data date field to datetime type\n",
    "training['date'] = pd.to_datetime(training['date'], format=\"%Y%m%d\")\n",
    "training = training.set_index('date').to_period('D')\n",
    "print(training.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_kg_hide-input": true,
    "papermill": {
     "duration": 342.485363,
     "end_time": "2021-06-07T23:30:07.356822",
     "exception": false,
     "start_time": "2021-06-07T23:24:24.871459",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " dict_keys(['nextDayPlayerEngagement', 'playerBoxScores'])\n"
     ]
    }
   ],
   "source": [
    "# Unpack nested dataframes and store in dictionary `training_dfs`\n",
    "training_dfs = unpack_data(training, dfs=dfs)\n",
    "print('\\n', training_dfs.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of Explainable AI will be easier to understand if we restrict our analysis to a single player. The next cell has a helper function to load data for only a single player, by default Aaron Judge of the NY Yankees, who had the highest overall engagement during the training period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_kg_hide-input": true,
    "papermill": {
     "duration": 5.517467,
     "end_time": "2021-06-07T23:30:12.899795",
     "exception": false,
     "start_time": "2021-06-07T23:30:07.382328",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pids_test = players.playerId.loc[\n",
    "    players.playerForTestSetAndFuturePreds.fillna(False)\n",
    "].astype(str)\n",
    "\n",
    "def make_playerBoxScores(dfs: dict, features):\n",
    "    X = dfs['playerBoxScores']\n",
    "    X = X[['gameDate', 'playerId'] + features]\n",
    "    # Create date index\n",
    "    X = X.rename(columns={'gameDate': 'date'})\n",
    "    X['date'] = pd.PeriodIndex(X.date, freq='D')\n",
    "    # Aggregate multiple games per day by summing\n",
    "    X = X.groupby(['date', 'playerId'], as_index=False).sum()\n",
    "    return X\n",
    "\n",
    "\n",
    "def make_targets(training_dfs: dict):\n",
    "    Y = training_dfs['nextDayPlayerEngagement'].copy()\n",
    "    # Match target dates to feature dates and create date index\n",
    "    Y = Y.rename(columns={'engagementMetricsDate': 'date'})\n",
    "    Y['date'] = pd.to_datetime(Y['date'])\n",
    "    Y = Y.set_index('date').to_period('D')\n",
    "    Y.index = Y.index - 1\n",
    "    return Y.reset_index()\n",
    "\n",
    "\n",
    "def join_datasets(dfs):\n",
    "    dfs = [x.pivot(index='date', columns='playerId') for x in dfs]\n",
    "    df = pd.concat(dfs, axis=1).stack().reset_index('playerId')\n",
    "    return df\n",
    "\n",
    "\n",
    "def make_training_data(training_dfs: dict,\n",
    "                       features,\n",
    "                       targets,\n",
    "                       fourier=4,\n",
    "                       player=592450,  # Aaron Judge of the NY Yankees\n",
    "                       test_size=30):\n",
    "    # Process dataframes\n",
    "    X = make_playerBoxScores(training_dfs, features)\n",
    "    Y = make_targets(training_dfs)\n",
    "    # Merge for processing\n",
    "    df = join_datasets([X, Y])\n",
    "    # Process\n",
    "    df = df.astype({'playerId': str})\n",
    "    df = df.astype({name: np.float32 for name in features})\n",
    "    # Filter for chosen player\n",
    "    df = df.loc[df.playerId == str(player), :].drop('playerId', axis=1)\n",
    "    for name in features:\n",
    "        df[name] = df[name].fillna(-1)\n",
    "    # Restore features and targets\n",
    "    X = df[features]\n",
    "    Y = df[targets]\n",
    "    # Create temporal features\n",
    "    fourier_terms = CalendarFourier(freq='A', order=fourier)\n",
    "    deterministic = DeterministicProcess(\n",
    "        index=X.index,\n",
    "        order=0,\n",
    "        seasonal=False,\n",
    "        additional_terms=[fourier_terms],\n",
    "    )\n",
    "    X = pd.concat([X, deterministic.in_sample()], axis=1)\n",
    "    # Create train / validation splits\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "        X,\n",
    "        Y,\n",
    "        test_size=test_size,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    return X_train, X_valid, y_train, y_valid, deterministic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've picked out a few features from the `playerBoxScores` dataframe, but there are lots more you could try (see the [data documentation](https://www.kaggle.com/c/mlb-player-digital-engagement-forecasting/data) for a complete description). Increase the number of Fourier components to model seasonality with in more detail. You could also look at explanations for other players -- the `players` dataframe can tell you the `playerId` for each player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_kg_hide-input": true,
    "papermill": {
     "duration": 0.121909,
     "end_time": "2021-06-07T23:30:13.044701",
     "exception": false,
     "start_time": "2021-06-07T23:30:12.922792",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "player = 592450  # Aaron Judge of the NY Yankees\n",
    "features = [\n",
    "    \"runsScored\",\n",
    "    \"hits\",\n",
    "    \"rbi\",\n",
    "    \"atBats\",\n",
    "    \"hitByPitch\",\n",
    "    \"saves\",\n",
    "    \"homeRuns\",\n",
    "    \"stolenBases\",\n",
    "    \"strikeOuts\",\n",
    "    \"groundIntoTriplePlay\",\n",
    "    \"totalBases\",\n",
    "]\n",
    "fourier = 4  # number of annual seasonal components\n",
    "\n",
    "targets = [\"target1\", \"target2\", \"target3\", \"target4\"]\n",
    "test_size = 30\n",
    "\n",
    "X_train, X_valid, y_train, y_valid, deterministic = make_training_data(\n",
    "    training_dfs, \n",
    "    features=features, \n",
    "    targets=targets,\n",
    "    fourier=4,\n",
    "    player=592450,\n",
    "    test_size=test_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.036549,
     "end_time": "2021-06-07T23:30:47.569193",
     "exception": false,
     "start_time": "2021-06-07T23:30:47.532644",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create a neural network with Keras and fit it to our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T23:30:47.64678Z",
     "iopub.status.busy": "2021-06-07T23:30:47.645816Z",
     "iopub.status.idle": "2021-06-07T23:30:47.72237Z",
     "shell.execute_reply": "2021-06-07T23:30:47.722849Z",
     "shell.execute_reply.started": "2021-06-07T23:06:36.852123Z"
    },
    "papermill": {
     "duration": 0.11699,
     "end_time": "2021-06-07T23:30:47.723031",
     "exception": false,
     "start_time": "2021-06-07T23:30:47.606041",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "INPUTS = X_train.shape[-1]\n",
    "OUTPUTS = y_train.shape[-1]\n",
    "early_stopping = keras.callbacks.EarlyStopping(patience=10,\n",
    "                                               restore_best_weights=True)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.InputLayer(name='numpy_inputs', input_shape=(INPUTS,)),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(OUTPUTS),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T23:30:47.809965Z",
     "iopub.status.busy": "2021-06-07T23:30:47.809281Z",
     "iopub.status.idle": "2021-06-07T23:31:32.828239Z",
     "shell.execute_reply": "2021-06-07T23:31:32.828768Z",
     "shell.execute_reply.started": "2021-06-07T23:06:49.511418Z"
    },
    "papermill": {
     "duration": 45.069407,
     "end_time": "2021-06-07T23:31:32.828944",
     "exception": false,
     "start_time": "2021-06-07T23:30:47.759537",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mae',\n",
    "    metrics=['mae'],\n",
    ")\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    epochs=50,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.224828,
     "end_time": "2021-06-07T23:31:33.281108",
     "exception": false,
     "start_time": "2021-06-07T23:31:33.05628",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Evaluate #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get an idea of how well our network was able to fit the data by plotting its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T23:31:33.73647Z",
     "iopub.status.busy": "2021-06-07T23:31:33.735508Z",
     "iopub.status.idle": "2021-06-07T23:31:35.133451Z",
     "shell.execute_reply": "2021-06-07T23:31:35.132904Z",
     "shell.execute_reply.started": "2021-06-07T23:07:55.049202Z"
    },
    "papermill": {
     "duration": 1.627791,
     "end_time": "2021-06-07T23:31:35.133596",
     "exception": false,
     "start_time": "2021-06-07T23:31:33.505805",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_fit = model.predict(X_train)\n",
    "y_fit = pd.DataFrame(y_fit,\n",
    "                     index=y_train.index,\n",
    "                     columns=y_train.columns)\n",
    "y_eval = model.predict(X_valid)\n",
    "y_eval = pd.DataFrame(y_eval,\n",
    "                      index=y_valid.index,\n",
    "                      columns=y_valid.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T23:31:35.623176Z",
     "iopub.status.busy": "2021-06-07T23:31:35.622535Z",
     "iopub.status.idle": "2021-06-07T23:31:37.411686Z",
     "shell.execute_reply": "2021-06-07T23:31:37.412191Z",
     "shell.execute_reply.started": "2021-06-07T23:07:57.512927Z"
    },
    "papermill": {
     "duration": 2.050107,
     "end_time": "2021-06-07T23:31:37.41236",
     "exception": false,
     "start_time": "2021-06-07T23:31:35.362253",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(11, 8))\n",
    "ax = y_eval.plot(ax=ax, subplots=True, color='C0')\n",
    "ax = y_valid.plot(subplots=True,\n",
    "                  sharex=True,\n",
    "                  ax=ax,\n",
    "                  color='0.25')\n",
    "ax = y_fit.plot(subplots=True,\n",
    "                sharex=True,\n",
    "                ax=ax,\n",
    "                color='C3')\n",
    "ax = y_train.plot(subplots=True,\n",
    "                  sharex=True,\n",
    "                  ax=ax,\n",
    "                  color='0.25')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.234556,
     "end_time": "2021-06-07T23:31:37.878617",
     "exception": false,
     "start_time": "2021-06-07T23:31:37.644061",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Explain #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explainable AI on Vertex AI Notebooks lets you compute feature attributions for neural networks. Feature attributions describe the contribution each features makes to the final prediction relative to a baseline. Feature attributions can help you tune your model by indicating which features are important and which are not. Features with little importance you could consider dropping from your feature set.\n",
    "\n",
    "Read more about Vertex Explainable AI here: [Introduction to Vertex Explainable AI for Vertex AI](https://cloud.google.com/vertex-ai/docs/explainable-ai/overview). In JupyterLab on Vertex AI Notebooks, you can also review a tutorial on XAI in the `tutorials` > `explainable_ai` > `sdk_tutorial.ipynb` file\n",
    "\n",
    "Now we can look at explanations using the `explainable_ai_sdk` library. Run the following cell on AI Notebooks with a Cloud TF image to see model explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T23:31:38.825566Z",
     "iopub.status.busy": "2021-06-07T23:31:38.824899Z",
     "iopub.status.idle": "2021-06-07T23:31:38.828073Z",
     "shell.execute_reply": "2021-06-07T23:31:38.827449Z"
    },
    "papermill": {
     "duration": 0.243921,
     "end_time": "2021-06-07T23:31:38.828208",
     "exception": false,
     "start_time": "2021-06-07T23:31:38.584287",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import explainable_ai_sdk\n",
    "from explainable_ai_sdk.model import configs\n",
    "from explainable_ai_sdk.metadata.tf.v2 import SavedModelMetadataBuilder\n",
    "\n",
    "model_path = \"working/model\"\n",
    "model.save(model_path)\n",
    "\n",
    "builder = SavedModelMetadataBuilder(model_path)\n",
    "builder.set_numeric_metadata(\n",
    "    \"numpy_inputs\",\n",
    "    input_baselines=[X_train.median().tolist()],  # attributions relative to the median of the target\n",
    "    index_feature_mapping=X_train.columns.tolist(),  # the names of each feature\n",
    ")\n",
    "builder.save_metadata(model_path)\n",
    "\n",
    "explainer = explainable_ai_sdk.load_model_from_local_path(\n",
    "    model_path=model_path,\n",
    "    config=configs.SampledShapleyConfig(path_count=20),\n",
    ")\n",
    "\n",
    "instances = list(X_valid.to_numpy())  # explanations for the validation set\n",
    "explanations = explainer.explain(instances={'numpy_inputs': instances})[0]  # needs to match name of `InputLayer` in the model\n",
    "\n",
    "explanations.visualize_attributions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a Feature Attributions chart above. Each of the attributions is computed on a given set of instances relative to a baseline, in our case the median on the training set. We've computed attributions for the validation set, the last 30 days of the data, but you could try a different set if you like by changing the `instances` parameter. To get attributions relative to another baseline (the mean, say), just change the computation for `input_baselines` above.\n",
    "\n",
    "Our feature attribution chart suggest that features like `atBats` and `strikeOuts` are important for digital engagement with Aaron Judge, but `stolenBases` much less so. We also see that the model did make use of several of the Fourier features modeling annual seasonality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning with Vizier #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second half of this notebook, we'll demonstrate **Vertex Vizier**, Vertex AI's hyperparameter optimization service. Among its capabilities are a *Bayesian Optimization* algorithm to search efficiently within a hyperparameter space, *transfer learning* to make use of information from previous hyperparameter studies, and *automated early stopping* when tuning models that train incrementally, like neural nets with stochastic gradient descent or gradient boosted trees. Google Research has a great whitepaper describing the capabilities of Vizier in detail: [Google Vizier: A Service for Black-Box Optimization](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/bcb15507f4b52991a0783013df4222240e942381.pdf). Also see the [Vizier guide](https://cloud.google.com/vertex-ai/docs/vizier/overview) for a nice overview.\n",
    "\n",
    "While in preview, Vertex Vizier is offered at [no charge](https://cloud.google.com/vertex-ai/pricing#vizier).\n",
    "\n",
    "## Setup ##\n",
    "\n",
    "First, make sure you've run everything in the notebook prior to this up to **Data Pipeline**.\n",
    "\n",
    "Now run the next cell to enable the Vertex AI API on your project and install the Python client in this notebook instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud services enable aiplatform.googleapis.com\n",
    "pip install -q google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell imports the Vizier service and defines some helper functions we'll use to run the hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "from typing import List, Dict\n",
    "\n",
    "import proto\n",
    "from google.cloud.aiplatform_v1beta1 import VizierServiceClient\n",
    "\n",
    "\n",
    "def create_study(\n",
    "    parameters: List[Dict],\n",
    "    metrics: List[Dict],\n",
    "    vizier_client,\n",
    "    project_id: str,\n",
    "    location: str = 'us-central1',\n",
    "):\n",
    "    parent = f\"projects/{project_id}/locations/{location}\"\n",
    "    display_name = \"{}_study_{}\".format(\n",
    "        project_id.replace(\"-\", \"\"),\n",
    "        datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
    "    study = {\n",
    "        'display_name': display_name,\n",
    "        'study_spec': {\n",
    "            # 'ALGORITHM_UNSPECIFIED' means Bayesian optimization\n",
    "            # can also be 'GRID_SEARCH' or 'RANDOM_SEARCH'\n",
    "            'algorithm': 'ALGORITHM_UNSPECIFIED',\n",
    "            'parameters': parameters,\n",
    "            'metrics': metrics,\n",
    "        }\n",
    "    }\n",
    "    study = vizier_client.create_study(parent=parent, study=study)\n",
    "    return study.name\n",
    "\n",
    "\n",
    "def params_to_dict(parameters):\n",
    "    return {p.parameter_id: p.value for p in parameters}\n",
    "\n",
    "\n",
    "def run_study(\n",
    "    metric_fn,\n",
    "    requests: int,\n",
    "    suggestions_per_request: int,\n",
    "    client_id: str,\n",
    "    study_id: str,\n",
    "    vizier_client,\n",
    "):\n",
    "    for k in range(requests):\n",
    "        suggest_response = vizier_client.suggest_trials({\n",
    "            \"parent\": study_id,\n",
    "            \"suggestion_count\": suggestions_per_request,\n",
    "            \"client_id\": client_id,\n",
    "        })\n",
    "        print(f\"Request {k}\")\n",
    "        for suggested_trial in suggest_response.result().trials:\n",
    "            suggested_params = params_to_dict(suggested_trial.parameters)\n",
    "            metric = metric_fn(suggested_params)\n",
    "            print(f\"Trial Results: {metric['metric_id']}={metric['value']}\")\n",
    "\n",
    "            vizier_client.add_trial_measurement({\n",
    "                'trial_name': suggested_trial.name,\n",
    "                'measurement': {\n",
    "                    'metrics': [metric]\n",
    "                }\n",
    "            })\n",
    "\n",
    "            response = vizier_client.complete_trial({\n",
    "                \"name\": suggested_trial.name,\n",
    "                \"trial_infeasible\": False\n",
    "            })\n",
    "\n",
    "\n",
    "def get_optimal_trials(study_id, vizier_client):\n",
    "    optimal_trials = vizier_client.list_optimal_trials({'parent': study_id})\n",
    "    optimal_params = []\n",
    "    for trial in proto.Message.to_dict(optimal_trials)['optimal_trials']:\n",
    "        optimal_params.append({p['parameter_id']: p['value'] for p in trial['parameters']})\n",
    "    return optimal_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create the client that will communicate with the Vizier service. You'll need to provide it with your project id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#JF code to read the project ID rather than manual entry\n",
    "PROJECT_ID_ARR=!gcloud config get-value project \n",
    "PROJECT_ID=PROJECT_ID_ARR[0]\n",
    "REGION = \"us-central1\"\n",
    "ENDPOINT = REGION + \"-aiplatform.googleapis.com\"\n",
    "vizier_client = VizierServiceClient(client_options=dict(api_endpoint=ENDPOINT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't know your project ID, you may be able to retrieve it with this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joshua-playground\n"
     ]
    }
   ],
   "source": [
    "!gcloud config list --format 'value(core.project)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Hyperparameters ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The search space (or feasible space) for each hyperparameter is determined by its *type*. In Vizier, hyperparameters can be one of four types: `DOUBLE` or `INTEGER`, defined by minimum and maximum values, or, `CATEGORICAL` or `DISCRETE`, defined by an enumeration of values. We've created one of each below for illustration. \n",
    "\n",
    "Hyperparameters of numeric type may also have their feasible space [scaled](https://cloud.google.com/vertex-ai/docs/reference/rest/v1beta1/StudySpec#scaletype). It's common to apply logarithmic scaling when you want to search efficiently across orders of magnitude, like for a learning rate between `1e-6` and `1e-1`. See the [`StudySpec` API documentation](https://cloud.google.com/vertex-ai/docs/reference/rest/v1beta1/StudySpec) for more about scaling and other options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter feasible space\n",
    "parameters = [{\n",
    "    'parameter_id': 'batch_size',\n",
    "    'integer_value_spec': {\n",
    "        'min_value': 8,\n",
    "        'max_value': 128,\n",
    "    }\n",
    "}, {\n",
    "    'parameter_id': 'dropout',\n",
    "    'double_value_spec': {\n",
    "        'min_value': 0.0,\n",
    "        'max_value': 0.5,\n",
    "    }\n",
    "}, {\n",
    "    'parameter_id': 'units',\n",
    "    'discrete_value_spec': {\n",
    "        'values': [128, 256, 512, 1024]\n",
    "    }\n",
    "}, {\n",
    "    'parameter_id': 'optimizer',\n",
    "    'categorical_value_spec': {\n",
    "        'values': ['sgd', 'adam'],\n",
    "    }\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-14T18:46:19.220237Z",
     "iopub.status.busy": "2021-06-14T18:46:19.219453Z",
     "iopub.status.idle": "2021-06-14T18:46:19.226889Z",
     "shell.execute_reply": "2021-06-14T18:46:19.225581Z",
     "shell.execute_reply.started": "2021-06-14T18:46:19.220042Z"
    }
   },
   "source": [
    "## Create Study ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create a *study*. A study conducts *trials* in order to optimize one or more *metrics*. A trial is a selection of hyperparameter values together with the outcome they produce. In our case, the hyperparameters define a neural net architechture and training regimen, and will produce a validation loss, the metric we hope to minimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [{\n",
    "    'metric_id': 'val_loss',  # the name of the quantity we want to minimize\n",
    "    'goal': 'MINIMIZE',  # choose MINIMIZE or MAXIMIZE\n",
    "}]\n",
    "\n",
    "# Call a helper function to create the study\n",
    "study_id = create_study(\n",
    "    parameters=parameters,\n",
    "    metrics=metrics,\n",
    "    vizier_client=vizier_client,\n",
    "    project_id=PROJECT_ID,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll put our model definition and training inside a function we can pass hyperparameters to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_fn(params):\n",
    "    # Parse hyperparameters\n",
    "    units = int(params['units'])\n",
    "    dropout = params['dropout']\n",
    "    optimizer = params['optimizer']\n",
    "    batch_size = int(params['batch_size'])\n",
    "    # Create and train model\n",
    "    INPUTS = X_train.shape[-1]\n",
    "    OUTPUTS = y_train.shape[-1]\n",
    "    early_stopping = keras.callbacks.EarlyStopping(patience=10,\n",
    "                                                   restore_best_weights=True)\n",
    "    model = keras.Sequential([\n",
    "        layers.InputLayer(name='numpy_inputs', input_shape=(INPUTS, )),\n",
    "        layers.Dense(units, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(dropout),\n",
    "        layers.Dense(units, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(dropout),\n",
    "        layers.Dense(units, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(dropout),\n",
    "        layers.Dense(OUTPUTS),\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='mae',\n",
    "        metrics=['mae'],\n",
    "    )\n",
    "    model.fit(X_train,\n",
    "              y_train,\n",
    "              validation_data=(X_valid, y_valid),\n",
    "              batch_size=batch_size,\n",
    "              epochs=50,\n",
    "              callbacks=[early_stopping],\n",
    "              verbose=0)\n",
    "    # Optimize the metric monitored by `early_stopping` (`val_loss` by default)\n",
    "    # The metric needs to be reported in this format\n",
    "    return {'metric_id': early_stopping.monitor, 'value': early_stopping.best}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That done, we'll call another helper function to run the hyperparameter search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Study ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-14 11:21:49.398889: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-03-14 11:21:49.399029: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-03-14 11:21:49.399063: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (python-20211226-110421): /proc/driver/nvidia/version does not exist\n",
      "2022-03-14 11:21:49.419237: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial Results: val_loss=10.528620719909668\n",
      "Request 1\n",
      "Trial Results: val_loss=10.843750953674316\n",
      "Request 2\n",
      "Trial Results: val_loss=10.663044929504395\n",
      "Request 3\n",
      "Trial Results: val_loss=10.287182807922363\n",
      "Request 4\n",
      "Trial Results: val_loss=11.229575157165527\n"
     ]
    }
   ],
   "source": [
    "run_study(\n",
    "    metric_fn,\n",
    "    requests=5,\n",
    "    # set >1 to get suggestions \"in parallel\", good for distributed training\n",
    "    suggestions_per_request=1,\n",
    "    # keep the name the same to resume a trial\n",
    "    client_id=\"client_1\",\n",
    "    study_id=study_id,\n",
    "    vizier_client=vizier_client,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at your [**Studies**](https://console.cloud.google.com/vertex-ai/experiments/studies) tab and select your study for a summary of the results. Also be sure to click the **Analysis** tab for the results presented in a parallel coordinates chart.\n",
    "\n",
    "Let's finish by looking at the best set of hyperparameters we found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_optimal_trials(study_id, vizier_client))"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "venv3",
   "name": "common-cpu.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m87"
  },
  "kernelspec": {
   "display_name": "venv3",
   "language": "python",
   "name": "venv3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
